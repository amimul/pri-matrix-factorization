{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jungle video trap challenge solution by AVeysov and SKolbachev\n",
    "==============================\n",
    "\n",
    "- Classifying animals using jungle trap videos (200k+, 1TB) with 90%+ accuracy with CNNs;\n",
    "- Also visit out [blog](https://spark-in.me/tag/group-data-science) and [channel](https://t.me/snakers4) (seriously!);\n",
    "\n",
    "To replicate our final result from [here](https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/leaderboard/) you need more or less to follow these steps:\n",
    "\n",
    "1. Download the dataset from [here](https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/) - (see some hints below);\n",
    "2. Build the environment via the below Dockerfile;\n",
    "3. Extract the features and metadata from the videos;\n",
    "4. Run the final models;\n",
    "\n",
    "\n",
    "Project Organization\n",
    "------------\n",
    "\n",
    "    ├── LICENSE\n",
    "    ├── README.md           <- The top-level README for developers using this project.\n",
    "    ├── data\n",
    "    │   ├── interim         <- Extracted features and metadata\n",
    "    │   ├── micro           <- The original micro dataset (64x64 videos)\n",
    "    │   └── raw             <- The original unpacked 1TB raw full size video dataset\n",
    "    │\n",
    "    ├── models              <- Trained and serialized models, model predictions, or model summaries\n",
    "    │\n",
    "    ├── notebooks           <- Jupyter notebooks (provided just fyi for completeness)\n",
    "    │\n",
    "    ├── Dockerfile          <- The Dockerfile to build the environment\n",
    "    │\n",
    "    ├── src                 <- Source code for use in this project.\n",
    "    │   ├── extract_features    <- Extracts CNN features\n",
    "    │   └── extract_meta_data   <- Extracts video meta-data\n",
    "    │\n",
    "    └── test_environment.py <- A set of small scripts to test the environment\n",
    "\n",
    "Downloading data\n",
    "------------\n",
    "\n",
    "Download and unpack the following datasets to their respective folders from [here](https://www.drivendata.org/competitions/49/deep-learning-camera-trap-animals/data/):\n",
    "- data/raw - the full 1TB dataset\n",
    "- data/micro - micro dataset\n",
    "\n",
    "Download the annotation files and micro dataset using the below jupyter notebook snippet:\n",
    "\n",
    "```\n",
    "import collections\n",
    "\n",
    "file_dict = collections.OrderedDict()\n",
    "file_dict['data/submission.csv'] = 'https://s3.amazonaws.com/drivendata/data/49/public/submission_format.csv'\n",
    "file_dict['data/train.csv'] = 'https://s3.amazonaws.com/drivendata/data/49/public/train_labels.csv'\n",
    "file_dict['data/micro/micro_chimps.tgz'] = 'https://s3.amazonaws.com/drivendata-public-assets/micro_chimps.tgz'\n",
    "\n",
    "for file,url in file_dict.items():\n",
    "    url_q = \"'\" + url + \"'\"\n",
    "    ! wget --continue --no-check-certificate --no-proxy -O $file $url_q\n",
    "\n",
    "```\n",
    "\n",
    "For the full dataset you will have to be more creative, I recommend using [this](https://help.ubuntu.com/community/TransmissionHowTo).\n",
    "\n",
    "Setting up the environment\n",
    "------------\n",
    "\n",
    "Key dependencies:\n",
    "- Ubuntu 16.04;\n",
    "- CUDA + CUDNN + Nvidia GPU;\n",
    "- Docker and nvidia-docker;\n",
    "- This pre-trained model [zoo](https://github.com/Cadene/pretrained-models.pytorch);\n",
    "\n",
    "Make sure that you are familiar with Docker and building docker images from Dockerfiles and [nvidia-docker](https://github.com/NVIDIA/nvidia-docker).\n",
    "\n",
    "Use the provided Dockerfile to build an environment.\n",
    "Change the **ENTER_YOUR_PASS_HERE** placeholder to the desired root password (is necessary if you want to ssh remotely into the container).\n",
    "\n",
    "The following libraries may have to be installed manually by logging into the container:\n",
    "- ```pip3 install moviepy```\n",
    "- ```pip3 install git+https://github.com/aleju/imgaug```\n",
    "- ```pip3 install sk-video```\n",
    "\n",
    "If you are not familiar with Docker, consider following / reading these materials:\n",
    "- [Docker](https://towardsdatascience.com/how-docker-can-help-you-become-a-more-effective-data-scientist-7fc048ef91d5) for data science;\n",
    "- A series of posts on our channel with useful links [1](https://t.me/snakers4/1476) [2](https://t.me/snakers4/1479) (if you do not speak Russian - just follow links - they ultimately link to Enlish articles mostly);\n",
    "\n",
    "**Warning**\n",
    "On some machines jupyter extenstions give me hell during installation, or they just do no work.\n",
    "Alternative ways of intallation\n",
    "\n",
    "```\n",
    "# A NOTE ON ENABLING JPN EXTENSIONS\n",
    "# sometimes pip install does not work on some systems\n",
    "# installation from source solves the problem\n",
    "git clone https://github.com/ipython-contrib/jupyter_contrib_nbextensions.git\n",
    "pip install -e jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --system\n",
    "\n",
    "# or install via pip from repository\n",
    "pip install git+https://github.com/ipython-contrib/jupyter_contrib_nbextensions\n",
    "jupyter contrib nbextension install --system\n",
    "```\n",
    "\n",
    "\n",
    "Running the environment\n",
    "------------\n",
    "\n",
    "1. Build the container;\n",
    "2. Make sure you have all the necessary ports exposed (i.e. 8888 for jupyter and 6006 for tensorboard);\n",
    "3. Use the below commands to run the container;\n",
    "4. Use ```docker exec -it --user root CONTAINER_ID /bin/bash``` to ssh into docker locally;\n",
    "5. You may also add ssh port pass-though (into EXPOSE statement and into docker run) and ssh into container remotely. Also you may change password auth to ssh-key;\n",
    "\n",
    "For nvidia-docker use:\n",
    "\n",
    "```\n",
    "nvidia-docker run -it -v /PATH/TO/YOUR/FOLDER:/home/keras/notebook -p 8888:8888 -p 6006:6006 --shm-size 8G YOUR_DOCKER_IMAGE_NAME --runtime=nvidia \n",
    "```\n",
    "\n",
    "For nvidia-docker 2 use:\n",
    "\n",
    "```\n",
    "docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all -it -v /PATH/TO/YOUR/FOLDER:/home/keras/notebook -p 8888:8888 -p 6006:6006 --shm-size 8G YOUR_DOCKER_IMAGE_NAME    \n",
    "```\n",
    "\n",
    "Note that without ```--shm-size``` Pytorch data loaders will not work.\n",
    "\n",
    "Testing the environment\n",
    "------------\n",
    "\n",
    "1. Run test_environment.py - and make sure it works;\n",
    "2. The only key major change that occured recently is Pytorch 0.3 being released, but this should not cause any issues - but be cautious;\n",
    "3. Not all keras + tf versions are friednly with each other - run at least one test from [here](https://github.com/nerox8664/pytorch2keras) to make sure error is low;\n",
    "\n",
    "\n",
    "Extracting features and meta-data\n",
    "------------\n",
    "\n",
    "You may consider fine-tuning the encoders (which performed strangely on validation for us - and therefore we failed to utilize them fully, though Dmytro did despite their validation performance) or building and end2end pipeline (which we also did), but the easiest and fastest way is to \n",
    "- extract features\n",
    "- use them as inputs to meta-model\n",
    "\n",
    "Run the following scripts sequentially. Note that on 2 Nvidia 1080 Ti GPUs each feature set (train+test) took ~ 1 day to unpack.\n",
    "\n",
    "- Extract meta-data\n",
    "```\n",
    "python3 src/extract_meta_data/extract_meta_data.py\n",
    "```\n",
    "- Create folders\n",
    "```\n",
    "mkdir -p ../../data/interim/inception4_test\n",
    "mkdir -p ../../data/interim/inception4_train\n",
    "\n",
    "mkdir -p ../../data/interim/ir_test\n",
    "mkdir -p ../../data/interim/ir_train\n",
    "\n",
    "mkdir -p ../../data/interim/nasnet_test\n",
    "mkdir -p ../../data/interim/nasnet_train\n",
    "\n",
    "mkdir -p ../../data/interim/resnet_test\n",
    "mkdir -p ../../data/interim/resnet_train\n",
    "\n",
    "```\n",
    "- Extract features via running these python scripts\n",
    "```\n",
    "python3 src/extract_features/inception4_extract_test.py\n",
    "python3 src/extract_features/inception4_extract_train.py\n",
    "python3 src/extract_features/inception_resnet_extract_test.py\n",
    "python3 src/extract_features/inception_resnet_extract_train.py\n",
    "python3 src/extract_features/nasnet_extract_test.py\n",
    "python3 src/extract_features/nasnet_extract_train.py\n",
    "python3 src/extract_features/resnet_extract_test.py\n",
    "python3 src/extract_features/resnet_extract_train.py\n",
    "```\n",
    "\n",
    "\n",
    "Training the final model \n",
    "------------\n",
    "[SK to fill]\n",
    "\n",
    "Supplementary materials \n",
    "------------\n",
    "Provided just fyi, use at your own risk.\n",
    "\n",
    "---\n",
    "\n",
    "Notebooks are best opened with the following [extensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions) enabled:\n",
    "![extentions](notebooks/extensions.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notebooks:\n",
    "- jungle.ipynb - data expoloration + various pipelines and experiments\n",
    "- playing_with_extractors.ipynb - playing with Pytorch feature extractors\n",
    "- video_loading_benchmark.ipynb - playing with several video processing libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
